<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> nicolabs | Speed up your automated Docker builds with GitHub Actions </title> <meta name="description" content=" Work in progress... "> <meta name="keywords" content="android, development, java, javascript, python, web"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Facebook / Open Graph --> <meta property="og:type" content="article"> <meta property="article:author" content="nicobo"> <meta property="article:section" content=""> <meta property="article:tag" content=""> <meta property="article:published_time" content="2021-03-22 22:17:44 +0100"> <meta property="og:url" content="https://www.nicolabs.net/2021/Speed-up-automated-docker-build-with-github-actions"> <meta property="og:title" content=" nicolabs | Speed up your automated Docker builds with GitHub Actions "> <meta property="og:image" content="https://www.nicolabs.net"> <meta property="og:description" content=" Work in progress... "> <meta property="og:site_name" content="nicobo"> <meta property="og:locale" content="en_US"> <!-- Social: Twitter --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="nic0b0"> <meta name="twitter:title" content=" nicolabs | Speed up your automated Docker builds with GitHub Actions "> <meta name="twitter:description" content=" Work in progress... "> <meta name="twitter:image:src" content="https://www.nicolabs.net"> <!-- Social: Google+ / Schema.org --> <meta itemprop="name" content=" nicolabs | Speed up your automated Docker builds with GitHub Actions "> <meta itemprop="description" content=" Work in progress... "> <meta itemprop="image" content="https://www.nicolabs.net"> <!-- rel prev and next --> <link rel="stylesheet" href="/assets/css/main.css"> <!-- Canonical link tag --> <link rel="canonical" href="https://www.nicolabs.net/2021/Speed-up-automated-docker-build-with-github-actions"> <link type="application/atom+xml" rel="alternate" href="https://www.nicolabs.net/feed.xml" title="nicolabs" /> <script type="text/javascript"> var disqus_shortname = 'nicolabs'; </script> <!-- Enable displaying pictures in full size using the Fullscreen API --> <!-- A polyfill that also simplifies the API. TODO maybe there are others closer to the norm and with more features. Still chances are this will not work on iPhone without using a full-fledged Js library. --> <script src="/assets/lib/screenfull.js/dist/screenfull.min.js"></script> <!-- This code selects which elements and how fullscreen is triggered --> <script> document.addEventListener("DOMContentLoaded", function(event) { var els = document.getElementsByClassName("plantuml"); for ( var e=0 ; e<els.length ; e++ ) { var el = els[e]; el.addEventListener('click', function() { if (screenfull.isEnabled) { screenfull.toggle(el); el.classList.toggle("fullscreen"); } else { console.log("Fullscreen not supported"); } }); } }); </script> </head> <body> <main class="wrapper"> <header class="site-header"> <nav class="nav"> <div class="container"> <h1 class="logo"><a href="/">nico<span>labs</span></a></h1> <ul class="navbar"> <li><a href="/about">about</a></li> <li><a href="/tags">tags</a></li> <li><a href="/feed.xml">feed</a></li> </ul> </div> </nav> </header> <article class="post container" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title" itemprop="name headline">Speed up your automated Docker builds with GitHub Actions</h1> <p class="post-meta"> <a href="https://github.com/nicolabs/nicolabs.github.io/commits/master/_preposts/Speed-up-automated-docker-build-with-github-actions.md" title="Read full history of this post"> <time class="datePublished" datetime="2021-03-22T22:17:44+01:00" itemprop="datePublished">Mar 22, 2021</time> </a> <span class="post-meta-separator">•</span> <span itemprop="read_time"> 13 minutes read </span> <span class="post-meta-separator">•</span> <span itemprop="maturity"><a href="/2016/Migrating-from-Drupal-to-Jekyll" title="Maturity of this article : draft < good < stable or deprecated&#xa;Click for the explanation.">Maturity : <span class="maturity-label maturity-draft" title="Maturity of this article : draft < good < stable or deprecated">draft</span> </span></a> </p> </header> <div class="post-content" itemprop="articleBody"> <p><img src="/assets/blog/3rdparty/pictures/happy_international_cat_day___by_bloglaurel_dbjdmqm.jpg" alt="Docker &amp; cats illustration by bloglaurel - https://www.deviantart.com/bloglaurel/art/Happy-International-Cat-Day-697676638" width="100%" /></p> <figcaption>Docker &amp; cats illustration by bloglaurel - https://www.deviantart.com/bloglaurel/art/Happy-International-Cat-Day-697676638</figcaption> <h2 id="introduction">Introduction</h2> <p>The <a href="https://docs.docker.com/ci-cd/github-actions/">universally</a> <a href="https://www.docker.com/blog/docker-github-actions/">advertized</a> <a href="https://docs.github.com/en/actions/guides/publishing-docker-images">way</a> of building Docker images with GitHub is to set up a <a href="https://docs.github.com/en/actions"><strong>GitHub Actions</strong></a> workflow.</p> <p><em>Github Actions</em> (GA) is actually very easy to use but nonetheless still <a href="https://github.com/actions/cache/graphs/code-frequency">under heavy development</a>.</p> <p>Unfortunately, almost all tutorials out there are based on (the same) very simplistic use cases. I just couldn’t get it right by simply following them : I’ve literally spent hours to test and understand how to <em>leverage the cache action for Docker multi-stage builds</em>.</p> <p>I hope this post will be useful to anyone with a similar use case.</p> <!--more--> <p>This article will <em>not</em> describe how to make your first GA workflow. We will look at <em>traps to avoid</em> when <strong>building multiple and multi-stage Docker images with GA</strong>, essentially covering caching, and more specifically using <em>actions/cache@v2</em>.</p> <h2 id="use-parallelism">Use parallelism</h2> <p>The first thing to take care of when building multiple images is to run tasks in parallel, whenever possible :</p> <ul> <li> <p>Docker’s <em>buildx</em> command already takes care of <a href="https://docs.docker.com/buildx/working-with-buildx/#build-multi-platform-images">multi-platform parallel building</a> so <strong>using <a href="https://github.com/docker/build-push-action">docker/build-push-action@v2</a> in your workflow is the way to go</strong>.</p> </li> <li> <p>You will also need to figure out how you can <strong>split your build in several <em>workflows</em> or <em>jobs</em></strong>. <a href="https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions">In GA, workflows run in parallel, as well as jobs inside a workflow</a>, by default.</p> </li> </ul> <p>Let’s consider my use case. I have 3 images : <em>alpine</em>, <em>debian</em> and <em>signal-debian</em>, from which the first one : <em>alpine</em>, has no dependency on the 2 others.</p> <p>Building them sequentially (as subsequent <em>steps</em> in a <em>job</em>) resulted in 1h50 runs… By simply building the <em>alpine</em> image in its own job <strong>I saved 40 min</strong> (the duration of the <em>alpine</em> build) !</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">jobs</span><span class="pi">:</span>

  <span class="na">build-publish-alpine</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">Build, Publish alpine</span>
    <span class="na">environment</span><span class="pi">:</span> <span class="s">prod</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">steps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout</span>
      <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>

  <span class="s">...</span>

  <span class="c1"># This job will run in parallel of build-push-alpine</span>
  <span class="na">build-publish-debian</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">Build, Publish debian</span>
    <span class="na">environment</span><span class="pi">:</span> <span class="s">prod</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">steps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout</span>
      <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>
</code></pre></div></div> <p>I leave it up to you to look at <a href="https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobs">the documentation</a> :</p> <ul> <li>ordering with <a href="https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobsjob_idneeds">jobs.&lt;job_id&gt;.needs</a></li> <li>can mutualize variables, steps, outputs, …</li> </ul> <h2 id="use-caching">Use caching</h2> <p>Building Docker images on your local machine uses cache by default. If your Dockerfile is correctly crafted this DRASTICALLY enhances build time. In my case, building from scratch takes up hours. My first concern was therefore to ensure my Dockerfiles were always <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache">cache-optimized</a>.</p> <p>When building with GitHub Actions however, caching is not straightforward.</p> <p>Let’s start with a conceptual difference between :</p> <ol> <li> <p><strong>sharing cache during the build</strong> (e.g. image1 you just built which is also the base image of image2 in the <em>same</em> workflow) - doing this reduces build duration when using the same layers multiple times during the build (multi-arch builds probably do)</p> </li> <li> <p><strong>reusing cache from previous builds</strong> - when you push a small modification in your code and the images have to be rebuilt, you would probably be grateful to get back the cache from the previous push, which was from <em>a past</em> workflow</p> </li> </ol> <p>There are two ways to cache data with GitHub actions : <a href="https://docs.github.com/en/actions/guides/storing-workflow-data-as-artifacts">upload-artifact and download-artifacts</a> actions and <a href="https://github.com/actions/cache">actions/cache</a> action.</p> <p>Although <em>actions/upload-artifact</em> and <em>actions/download-artifact</em> might be able to cover both points above, it is not meant to cache docker layers. <a href="https://docs.github.com/en/actions/guides/caching-dependencies-to-speed-up-workflows#comparing-artifacts-and-dependency-caching">The major recommended approach I’ve seen</a> is to use <strong>actions/cache@v2</strong>, which also covers the two concepts.</p> <blockquote> <p><strong>NOTE</strong> In theory, the ‘artifact’ approach should also work, but is not advertized for this kind of usage. This is maybe the premises of a future update to this article.</p> </blockquote> <h3 id="set-modemax">Set mode=max</h3> <p>Common snippets found on the web for the <em>docker/build-push-action</em> <strong>will only cache final Docker images</strong> :</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build and push</span>
  <span class="na">id</span><span class="pi">:</span> <span class="s">docker_build</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@v2</span>
  <span class="na">with</span><span class="pi">:</span>
    <span class="na">context</span><span class="pi">:</span> <span class="s">./</span>
    <span class="na">file</span><span class="pi">:</span> <span class="s">./Dockerfile</span>
    <span class="na">builder</span><span class="pi">:</span> <span class="s">$</span>
    <span class="na">push</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">tags</span><span class="pi">:</span>  <span class="s">ushamandya/simplewhale:latest</span>
    <span class="na">cache-from</span><span class="pi">:</span> <span class="s">type=local,src=/tmp/.buildx-cache</span>
    <span class="na">cache-to</span><span class="pi">:</span> <span class="s">type=local,dest=/tmp/.buildx-cache</span>
</code></pre></div></div> <p>If you deal with multi-stage Dockerfiles, you MUST absolutely <a href="https://github.com/docker/buildx#--cache-tonametypetypekeyvalue">set <code class="highlighter-rouge">mode=max</code> attribute on the <code class="highlighter-rouge">cache-to</code> entry to enable caching of layers from all stages</a> :</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="na">cache-to</span><span class="pi">:</span> <span class="s">type=local,dest=/tmp/.buildx-cache,mode=max</span>
</code></pre></div></div> <p>For me, this reduced the build time of individual jobs using the cache <strong>from ~45 minutes to 5-10 minutes !</strong></p> <h3 id="there-is-an-overhead">There is an overhead</h3> <p>With this technique, subsequent runs of the same job will still vary (this is why I’ve stated a 5-10 min. variation above). It is due to <em>actions/cache</em>, which saves and retrieves the cache from a remote storage (S3 or alike). <strong>The more the cache grows, the longer it takes to get and save it.</strong></p> <p>I’ve observed varying overheads from 1 min. for a 2GB cache, up to 7 min. for a 4GB+ cache (including both download and upload). Repeat this for every job…</p> <p>The exact pattern depends on what you put in the cache but this is something you should definitely keep in mind, as caching may not be worth it, for instance if your build is fast and uses a large cache.</p> <h3 id="there-is-a-size-limit">There is a size limit</h3> <p>, if it is present or absent/has been deleted as per the GH retention policy (in which case it’s fast because it’s initialized).</p> <p>Don’t be too greedy on cache reuse because if may lead a lot of steps to use the same cache, and therefore it will never be released.</p> <p>For instance in this example, if the step <em>docker_build_debian</em> fills the cache with 5GB, it will be the only cache as it uses all the allowed space. The <em>docker_build_alpine</em> will likely not be able to add more content to the cache or it will be the whole cache will be discarded later by github, forcing the next build to start from scratch.</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Cache Docker layers</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/cache@v2</span>
  <span class="na">with</span><span class="pi">:</span>
      <span class="na">path</span><span class="pi">:</span> <span class="s">/tmp/.buildx-cache</span>
      <span class="na">key</span><span class="pi">:</span> <span class="s">$-buildx-debian-$</span>
      <span class="na">restore-keys</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">$-buildx-debian-</span>
        <span class="s">$-buildx-</span>

<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build and push debian</span>
  <span class="na">id</span><span class="pi">:</span> <span class="s">docker_build_debian</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@v2</span>
  <span class="na">with</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./</span>
      <span class="na">file</span><span class="pi">:</span> <span class="s">./debian.Dockerfile</span>
      <span class="na">builder</span><span class="pi">:</span> <span class="s">$</span>
      <span class="na">push</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">cache-from</span><span class="pi">:</span> <span class="s">type=local,src=/tmp/.buildx-cache</span>
      <span class="na">cache-to</span><span class="pi">:</span> <span class="s">type=local,dest=/tmp/.buildx-cache,mode=max</span>

<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build and push alpine</span>
  <span class="na">id</span><span class="pi">:</span> <span class="s">docker_build_alpine</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@v2</span>
  <span class="na">with</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./</span>
      <span class="na">file</span><span class="pi">:</span> <span class="s">./alpine.Dockerfile</span>
      <span class="na">builder</span><span class="pi">:</span> <span class="s">$</span>
      <span class="na">push</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">cache-from</span><span class="pi">:</span> <span class="s">type=local,src=/tmp/.buildx-cache</span>
      <span class="na">cache-to</span><span class="pi">:</span> <span class="s">type=local,dest=/tmp/.buildx-cache,mode=max</span>
</code></pre></div></div> <p>In the following execution trace of the <em>Cache Docker layers</em> step, we see that although we are in the section that builds the <em>debian</em> image (<code class="highlighter-rouge">key: Linux-buildx-debian-9176c5bd644818205d94a68221a7ebf27005b30e</code>), it hits the previous cache from the <em>alpine</em> image (<code class="highlighter-rouge">Cache restored from key: Linux-buildx-alpine-75bd3133c854ab90910b7ceb92445fafbe256260</code>), which has little chance to provide the layers it needs :</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Run actions/cache@v2
  with:
    path: /tmp/.buildx-cache
    key: Linux-buildx-debian-9176c5bd644818205d94a68221a7ebf27005b30e
    restore-keys: Linux-buildx-debian-
  Linux-buildx-

  env:
    pythonLocation: /opt/hostedtoolcache/Python/3.9.1/x64
    LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.9.1/x64/lib
    DEBIAN_TAGS: ***/nicobot:dev-debian
    SIGNAL_DEBIAN_TAGS: ***/nicobot:dev-signal-debian
    ALPINE_TAGS: ***/nicobot:dev-alpine
    NICOBOT_VERSION: 0.1.dev1
Received 67108864 of 4381819546 (1.5%), 63.8 MBs/sec
Received 218103808 of 4381819546 (5.0%), 103.8 MBs/sec
[...]
Cache Size: ~4179 MB (4381819546 B)
/bin/tar --use-compress-program zstd -d -xf /home/runner/work/_temp/3eafeb81-b970-4b87-9515-d659390d4387/cache.tzst -P -C /home/runner/work/nicobot/nicobot
Cache restored from key: Linux-buildx-alpine-75bd3133c854ab90910b7ceb92445fafbe256260
</code></pre></div></div> <p>If the 2 steps <em>docker_build_debian</em> and <em>docker_build_alpine</em> don’t need to share the same cache (here they have different base images and intermediate layers), a more efficient use of GA cache here would be to make sure they generate separate caches so they can be evicted separately. This can be done by limiting the <code class="highlighter-rouge">restore-keys</code> to non-overlapping values :</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Cache Docker layers</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/cache@v2</span>
  <span class="na">with</span><span class="pi">:</span>
      <span class="na">path</span><span class="pi">:</span> <span class="s">/tmp/.buildx-cache</span>
      <span class="na">key</span><span class="pi">:</span> <span class="s">$-buildx-debian-$</span>
      <span class="na">restore-keys</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">$-buildx-debian-</span>

<span class="s">...</span>

<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Cache Docker layers</span>
  <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/cache@v2</span>
  <span class="na">with</span><span class="pi">:</span>
      <span class="na">path</span><span class="pi">:</span> <span class="s">/tmp/.buildx-cache</span>
      <span class="na">key</span><span class="pi">:</span> <span class="s">$-buildx-alpine-$</span>
      <span class="na">restore-keys</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">$-buildx-alpine-</span>
</code></pre></div></div> <p>In the worst case, if your build always exceeds the cache limit, the cache will be discarded everytime and you will only be able to get some benefit from within a job, not between two subsequent runs.</p> <p>In other cases (after a week a cache has not been used or if you have several caches which don’t exceed the limit individually but overall do), you will observe longer builds from time to time because some caches have been evicted.</p> <h2 id="storage-size-limitations">Storage size limitations</h2> <p>On GH : 5GB for cache, 22 GB as observed for the Docker /var/lib (actually /).</p> <blockquote> <p><strong>NOTE</strong> : You may be able to overcome this limit with self-hosted runners, external storage (e.g. AWS), multiple GH projects (as the limits are per project), …</p> </blockquote> <p>In order to get full benefits of GH services it is possible to do some optimizations.</p> <p>Split in jobs when possible, to allow the cache to be released (it appears that cache eviction is not done while the runner is alive).</p> <p>Splitting in smaller jobs also reduce the chances that the filesystem grows to much during a monolothic step.</p> <h2 id="keep-up-with-the-flow">Keep up with the flow</h2> <p>It’s probably a good idea to include some sanity checks / actions in your jobs to be more future proof regarding new or recurring bugs.</p> <p>Example for Ubuntu :</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Sanity actions</span>
  <span class="na">run</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s"># Will print the filesystem mounts and available storage</span>
    <span class="s">sudo df -h</span>
    <span class="s"># May free up some place</span>
    <span class="s">sudo apt clean</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Out of the box, GA allows to quickly set up continuous integration workflows for very basic requirements.</p> <p>First make sure to design your workflows so that they <strong>take advantage of parallel building</strong>.</p> <p>Caching is another key factor to faster builds : the <strong>actions/cache@v2</strong> action can greatly improves the speed of the builds (x5 to x9). However, to use it right, this action requires a deep understanding of its internal workflow and is limited to 5GB in size, such that it loses most its interest if your build regularly exceeds this limit. You may also not need to use such a cache if your build only takes a few minutes to complete, as downloading &amp; saving the cache adds an overhead to the total execution time.</p> <p>This article showed some tips and workaround to get the maximum of it, but while the current <strong>cache is limited to 5GB</strong>, alternatives - like self-hosted runners, external storage, splitting in multiple github repositories or re-thinking about which cases should trigger a build at first - may be more suited for storage-demanding use cases like multi-images Docker builds.</p> <p>By combining all practices described here, <strong>I was able to speed up repeated builds of 3 complex images from 1h50 to 10 minutes</strong>.</p> <h2 id="references">References</h2> <ul> <li><a href="https://github.com/actions/cache">actions/cache</a></li> </ul> <p>https://github.com/whoan/docker-build-with-cache-action https://github.com/sdras/awesome-actions Storing &amp; retrieving <em>github artifacts</em>](https://docs.github.com/en/actions/guides/storing-workflow-data-as-artifacts)</p> <aside class="tags"> <ul class="tags"> <li class="tag"><a href="/tags#docker">#docker</a></li> <li class="tag"><a href="/tags#github">#github</a></li> <li class="tag"><a href="/tags#github-actions">#github actions</a></li> <li class="tag"><a href="/tags#ci">#ci</a></li> </ul> </aside> <aside class="share"> <strong>Share this :</strong> <a href="http://twitter.com/share?text=Speed up your automated Docker builds with GitHub Actions&amp;url=https://www.nicolabs.net/2021/Speed-up-automated-docker-build-with-github-actions&amp;hashtags=web,dev,blog,soudev&amp;via=nic0b0" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">Twitter</a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.nicolabs.net/2021/Speed-up-automated-docker-build-with-github-actions" onclick="window.open(this.href, 'facebook-share', 'width=550,height=235');return false;">Facebook</a> </aside> </div> </article> <footer class="site-footer"> <div class="container"> <small class="pull-left">&copy;2021 All rights reserved.</small> <small class="pull-right">by <a rel="me" href="/about#contact">@nicobo</a></small> </div> </footer> </main> </body> </html>
